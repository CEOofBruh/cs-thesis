{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eeyvee-0x4d/cs-thesis/blob/main/Notebook/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdQ3Ic7Yagi"
      },
      "source": [
        "<h1>Clone repository</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9WC5LBymdt",
        "outputId": "bf655210-9e0c-43c2-9982-cc0f06739174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaFgOkofymd4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "\n",
        "df = pd.read_csv('/gdrive/MyDrive/vaccination_all_tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6U4f5en07He"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['id', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'hashtags', 'source', 'retweets', 'favorites', 'is_retweet'])\n",
        "df.to_csv(path_or_buf='/content/tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEUhwLHG8JpF"
      },
      "outputs": [],
      "source": [
        "txtfile = open('/content/locations.txt', 'r')\n",
        "locations = txtfile.read().splitlines()\n",
        "\n",
        "df = pd.read_csv('/content/tweets.csv')\n",
        "tweets = df[df['user_location'].isin(locations)]\n",
        "tweets.to_csv(path_or_buf='/content/tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6V5eVRbMm5e",
        "outputId": "d04482e4-3b69-47d5-bea8-b0c63f7b7ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'thesis'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 282 (delta 71), reused 196 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (282/282), 42.24 MiB | 3.17 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/thesis\n",
            "Dataset  Notebook  README.md  StreamingAPI\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/eeyvee-0x4d/cs-thesis thesis\n",
        "%cd thesis\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RImCpBW1LApR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7jw9rEvakhs",
        "outputId": "ab30a4ca-5afd-40f1-f5f8-f2d6eaf9519c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Sentiment                                               Text\n",
            "0   Negative    oh my god sakita sa akong bukton uy astrazeneca\n",
            "1   Negative  nakakapanghina ng katawan ang vaccine nagkaron...\n",
            "2   Positive  second dose done heart astrazeneca dbcvaccinat...\n",
            "3   Negative  same lang din side effect ng first at second d...\n",
            "4   Positive  booster shot done resbakuna aztrazeneca-pfizer...\n",
            "5   Positive  got boosted today  astrazeneca covid booster b...\n",
            "6   Positive  just got boosted let us fight covid variants t...\n",
            "7   Positive  booster done astrazeneca resbakuna booster htt...\n",
            "8   Positive  yay mama and papa got their first shot today r...\n",
            "9   Positive  doing my civic and moral duty to protect mysel...\n",
            "10  Positive  just got boosted let us fight covid variants  ...\n",
            "11  Positive  booster done astrazeneca resbakuna booster htt...\n",
            "12  Negative  ang sakit ng buong katawan ko muscles and butu...\n",
            "13  Negative  got my first dose yan tapang ka kanina lagnat ...\n",
            "14  Positive  done second dose salamat sa dios astrazeneca m...\n",
            "15  Negative  antukin ako pero yung antok ko ngayon kakaiba ...\n",
            "16  Negative            lalagnatin ako shet astrazeneca booster\n",
            "17  Positive                    got my booster shot astrazeneca\n",
            "18  Positive  done with my booster shot astrazeneca https://...\n",
            "19  Negative  eleven eleven mawala na ang side effect ng ast...\n",
            "20  Positive  I am done with my booster shot pa-booster na d...\n",
            "21  Positive  finally got my astrazeneca booster jab today h...\n",
            "22  Negative  ready na tayo lagnatin mamaya booster shot ast...\n",
            "23  Negative  para akong binugbog ni astrazeneca https://t.c...\n",
            "24  Negative  napaka traydor mo naman astrazeneca booster ht...\n",
            "25  Negative  that booster jab yesterday makes me sick at na...\n",
            "26  Negative  putang inang booster sama ng pakiramdam ko hig...\n",
            "27  Negative  para akong nakipag boxing ng three rounds saki...\n",
            "28  Negative  shems ramdam ko na ang sumpa ni zenki sana buk...\n",
            "29  Positive  na-booster na astrazeneca gid hahaha https://t...\n",
            "30  Negative  wala ka pang ten minutes ambigat mo na  ? Boos...\n",
            "31  Negative                    fever astrazeneca booster shots\n",
            "32  Positive  went to nbs after being vaccinated went out em...\n",
            "33  Positive  let us keep that protection longer get boosted...\n",
            "34  Negative  yung ang sama ng pakiramdam ko dahil  sa vacci...\n",
            "35  Negative  gagi from second dose to booster ng astrazenec...\n",
            "36  Negative  ah yawa sakit ng buong katawan ko astrazeneca ...\n",
            "37  Negative  ang sakit ng bewang ko sexbomb sexbomb sexbomb...\n",
            "38  Negative                   tamang seizures lang astrazeneca\n",
            "39  Negative  grabe naman adverse effects ng booster shot as...\n",
            "40  Positive  just got boosted let us fight covid variants  ...\n",
            "41  Negative                  kangalay booster haha astrazeneca\n",
            "42  Negative  hindi ako nilagnat pero sinipon naman ako boos...\n",
            "43  Negative  ang sakit ng braso ko dahil sa booster astraze...\n",
            "44  Negative                  saket ng astrazeneca booster dose\n",
            "45  Negative  normal ba ito naninilaw yung tinurukan ng baku...\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/Astra-December.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXR9EvKZB0Y"
      },
      "source": [
        "<h1>Text Preprocessing</h1>\n",
        "<ul>\n",
        "  <li>Import dataset</li>\n",
        "  <li>Remove urls</li>\n",
        "  <li>Remove special characters</li>\n",
        "  <li>Convert text data to lowercase</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-VQ9aBDLDM3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/Astra-December.csv') #read csv\n",
        "\n",
        "df_shape = df.shape # (row, column)\n",
        "\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df_shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', df.at[i, 'Text']).lower()\n",
        "  df.at[i, 'Text'] = re.sub(r'\\n', '', string)\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDyDPZA-VV9"
      },
      "source": [
        "<h1>Natural Language Toolkit NLTK</h1>\n",
        "<p>\n",
        "Nltk will be used to preprocess to corpus.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhuqMaQU6Lwb",
        "outputId": "d719921f-6680-4563-86b8-28a1f77f6ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1TpyQH-1JW"
      },
      "source": [
        "<h1>Stemming words using Porter Stemmer</h1>\n",
        "<p>\n",
        "Apply porter stemmer to each tokens first then rebuild the tokens into sentence.\n",
        "</p>\n",
        "<hr>\n",
        "<h1>Stop words removal</h1>\n",
        "<p>\n",
        "Remove stop words in english and tagalog.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "WyqpJpwL6lYO"
      },
      "outputs": [],
      "source": [
        "corpus = df[\"Text\"]\n",
        "\n",
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "filtered_sentence = []\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "  document = corpus[i]\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens] # stem each words\n",
        "  filtered_sentence = [token for token in stemmed_tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  # remove tagalog stopwords\n",
        "\n",
        "  document = \" \".join(filtered_sentence)\n",
        "  corpus[i] = document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FN438rY_aOV"
      },
      "source": [
        "<h1>Create n-grams from 1-4 ?</h1>\n",
        "<p>\n",
        "Create n-grams from 1 to 4 and evaluate each.\n",
        "</p>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}