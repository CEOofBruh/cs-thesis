{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eeyvee-0x4d/cs-thesis/blob/main/Notebook/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdQ3Ic7Yagi"
      },
      "source": [
        "<h1>Clone repository</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6V5eVRbMm5e"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/eeyvee-0x4d/cs-thesis thesis\n",
        "%cd thesis\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhuqMaQU6Lwb"
      },
      "outputs": [],
      "source": [
        "!pip install stopwordsiso\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "import stopwordsiso\n",
        "\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "import re\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXR9EvKZB0Y"
      },
      "source": [
        "<h1>Text Preprocessing</h1>\n",
        "<ul>\n",
        "  <li>Import dataset</li>\n",
        "  <li>Remove urls</li>\n",
        "  <li>Remove special characters</li>\n",
        "  <li>Convert text data to lowercase</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "X-VQ9aBDLDM3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/training_data.csv') #read csv\n",
        "\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df.shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', df.at[i, 'Text']).lower()\n",
        "  string = re.sub(r'[^a-zA-Z0-9 ]', '', string)\n",
        "  string = re.sub(r'\\b\\w{1,3}\\b', '', string)\n",
        "  df.at[i, 'Text'] = re.sub(r'\\n', ' ', string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDyDPZA-VV9"
      },
      "source": [
        "<h1>Natural Language Toolkit NLTK</h1>\n",
        "<p>\n",
        "Nltk will be used to preprocess to corpus.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1TpyQH-1JW"
      },
      "source": [
        "<h1>Stemming words using Porter Stemmer</h1>\n",
        "<p>\n",
        "Apply porter stemmer to each tokens first then rebuild the tokens into sentence.\n",
        "</p>\n",
        "<hr>\n",
        "<h1>Stop words removal</h1>\n",
        "<p>\n",
        "Remove stop words in english and tagalog.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "WyqpJpwL6lYO"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "stopwords_tl  = set(stopwordsiso.stopwords('tl'))\n",
        "filtered_sentence = []\n",
        "filtered_sentence2 = []\n",
        "\n",
        "for i in range(len(df['Text'])):\n",
        "  document = df.loc[i, 'Text']\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  filtered_sentence = [token for token in tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  filtered_sentence2 = [token for token in filtered_sentence if not token in stopwords_tl] #remove tagalog stopwords\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in filtered_sentence2] # stem each words\n",
        "\n",
        "  document = \" \".join(stemmed_tokens)\n",
        "  df.loc[i, 'Text'] = document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FN438rY_aOV"
      },
      "source": [
        "# Create n-grams from 1-4\n",
        "\n",
        "Create n-grams from 1 to 4 for exprementational purposes. Use params `ngram_range=(1,1)`, `ngram_range=(1,2)`, `ngram_range=(1,3)`, `ngram_range=(1,4)` in `TfidfVectorizer(ngram_range=(1,1))`.\n",
        "Default is `ngram_range=(1,1)`\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Perform TF-IDF to the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['Text'] # tweets\n",
        "y = df['Sentiment'] # labels\n",
        "\n",
        "\"\"\"\n",
        "X_train = train data\n",
        "y_train = train data labels\n",
        "\n",
        "X_test = test data\n",
        "y_test = test data labels\n",
        "\"\"\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "1Lt5YDqcSMh2"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-gram\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1), min_df=10) # Initialize vectorizer\n",
        "train_data_features = vectorizer.fit_transform(X_train)\n",
        "test_data_features = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "wVysZhaQSQoi"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "x_train_oversampled, y_train_oversampled = oversample.fit_resample(train_data_features, y_train)"
      ],
      "metadata": {
        "id": "tL9EFzY1FxM5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters = {'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
        "parameters = {'alpha': [1]}\n",
        "\n",
        "scorers = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score)\n",
        "}\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "classifier = GridSearchCV(mnb, parameters, return_train_score=False, cv=10, scoring=scorers, refit='accuracy')\n",
        "classifier.fit(train_data_features, y_train)\n",
        "# classifier.fit(x_train_oversampled, y_train_oversampled)"
      ],
      "metadata": {
        "id": "ntsQZqyfnKKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(classifier.cv_results_)\n",
        "results[['param_alpha', 'mean_test_accuracy', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall', 'rank_test_accuracy']]"
      ],
      "metadata": {
        "id": "3kkXVJ0va9hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(test_data_features) # classifier predictions\n",
        "y_true = y_test # ground truth\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy} F1 Score: {f1} Precision: {precision} Recall: {recall}\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "print(f\"\\nConfusion Matrix: \\nTrue Negative: {tn} False Positive: {fp} False Negative: {fn} True Positive: {tp}\")"
      ],
      "metadata": {
        "id": "ZAYNYwASOcKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best = results.loc[results['rank_test_accuracy'] == 1]\n",
        "best[['param_alpha', 'mean_test_accuracy', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall', 'rank_test_accuracy']]\n",
        "\n",
        "model_stats = {\n",
        "  \"accuracy\": round(float(best['mean_test_accuracy'] * 100), 2),\n",
        "  \"precision\": round(float(best['mean_test_precision'] * 100), 2),\n",
        "  \"recall\": round(float(best['mean_test_recall'] * 100), 2),\n",
        "  \"f1score\": round(float(best['mean_test_f1'] * 100), 2)\n",
        "}\n",
        "\n",
        "# convert into JSON:\n",
        "model_stats = json.dumps(model_stats)\n",
        "\n",
        "with open(\"model_stats.json\", \"w\") as outfile:\n",
        "    outfile.write(model_stats)"
      ],
      "metadata": {
        "id": "GpJ2EQpsUUHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "UHvBOSVckSPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = classifier.predict(train_data_features)\n",
        "tn, fp, fn, tp = confusion_matrix(y_train, pred).ravel()\n",
        "print(f\"True Negative: {tn} False Positive: {fp} False Negative: {fn} True Postive: {tp}\")"
      ],
      "metadata": {
        "id": "ivDVtof5itTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29333110-2f6c-48d6-a26a-982fed124510"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Negative: 193 False Positive: 73 False Negative: 30 True Postive: 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(classifier, open('model.pkl', 'wb'))\n",
        "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "uW-TlfGz1az1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}