{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eeyvee-0x4d/cs-thesis/blob/main/Notebook/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdQ3Ic7Yagi"
      },
      "source": [
        "<h1>Clone repository</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9WC5LBymdt",
        "outputId": "bf655210-9e0c-43c2-9982-cc0f06739174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaFgOkofymd4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "\n",
        "df = pd.read_csv('/gdrive/MyDrive/vaccination_all_tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6U4f5en07He"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['id', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'hashtags', 'source', 'retweets', 'favorites', 'is_retweet'])\n",
        "df.to_csv(path_or_buf='/content/tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEUhwLHG8JpF"
      },
      "outputs": [],
      "source": [
        "txtfile = open('/content/locations.txt', 'r')\n",
        "locations = txtfile.read().splitlines()\n",
        "\n",
        "df = pd.read_csv('/content/tweets.csv')\n",
        "tweets = df[df['user_location'].isin(locations)]\n",
        "tweets.to_csv(path_or_buf='/content/tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6V5eVRbMm5e",
        "outputId": "ef8ef301-5f5b-47cd-c541-85a9fdc26e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'thesis'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 1061, done.\u001b[K\n",
            "remote: Counting objects: 100% (1061/1061), done.\u001b[K\n",
            "remote: Compressing objects: 100% (732/732), done.\u001b[K\n",
            "remote: Total 1061 (delta 422), reused 832 (delta 288), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1061/1061), 102.95 MiB | 17.42 MiB/s, done.\n",
            "Resolving deltas: 100% (422/422), done.\n",
            "/content/thesis\n",
            "client\tDataset  kaggle_tweets.csv  Notebook  README.md  server  StreamingAPI\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/eeyvee-0x4d/cs-thesis thesis\n",
        "%cd thesis\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RImCpBW1LApR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXR9EvKZB0Y"
      },
      "source": [
        "<h1>Text Preprocessing</h1>\n",
        "<ul>\n",
        "  <li>Import dataset</li>\n",
        "  <li>Remove urls</li>\n",
        "  <li>Remove special characters</li>\n",
        "  <li>Convert text data to lowercase</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X-VQ9aBDLDM3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/trainingdata.csv') #read csv\n",
        "\n",
        "df_shape = df.shape # (row, column)\n",
        "\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df_shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', df.at[i, 'Text']).lower()\n",
        "  string = re.sub(r'[^a-zA-Z0-9 ]', '', string)\n",
        "  df.at[i, 'Text'] = re.sub(r'\\n', ' ', string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDyDPZA-VV9"
      },
      "source": [
        "<h1>Natural Language Toolkit NLTK</h1>\n",
        "<p>\n",
        "Nltk will be used to preprocess to corpus.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhuqMaQU6Lwb",
        "outputId": "0a9b7c37-d8a6-406d-8389-cfdce6d60779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stopwordsiso\n",
            "  Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▌                           | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 40 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 51 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 61 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 71 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "!pip install stopwordsiso\n",
        "\n",
        "import nltk\n",
        "import stopwordsiso\n",
        "\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1TpyQH-1JW"
      },
      "source": [
        "<h1>Stemming words using Porter Stemmer</h1>\n",
        "<p>\n",
        "Apply porter stemmer to each tokens first then rebuild the tokens into sentence.\n",
        "</p>\n",
        "<hr>\n",
        "<h1>Stop words removal</h1>\n",
        "<p>\n",
        "Remove stop words in english and tagalog.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WyqpJpwL6lYO"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "stopwords_tl  = set(stopwordsiso.stopwords('tl'))\n",
        "filtered_sentence = []\n",
        "filtered_sentence2 = []\n",
        "\n",
        "for i in range(len(df['Text'])):\n",
        "  document = df.loc[i, 'Text']\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  filtered_sentence = [token for token in tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  filtered_sentence2 = [token for token in filtered_sentence if not token in stopwords_tl] #remove tagalog stopwords\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in filtered_sentence2] # stem each words\n",
        "\n",
        "  document = \" \".join(stemmed_tokens)\n",
        "  df.loc[i, 'Text'] = document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FN438rY_aOV"
      },
      "source": [
        "# Create train data and test data\n",
        "\n",
        "Create train data and test data using 2:1 ration. Use `train_test_split` for this.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Create n-grams from 1-4 ?\n",
        "\n",
        "Create n-grams from 1 to 4 for exprementational purposes. Use params `ngram_range=(1,1)`, `ngram_range=(1,2)`, `ngram_range=(1,3)`, `ngram_range=(1,4)` in `TfidfVectorizer(ngram_range=(1,1))`.\n",
        "Default is `ngram_range=(1,1)`\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Perform TF-IDF to the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "wKLdAMszBeEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b2bc6c-50cf-4057-eccb-c259d28fcc43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment']) # split dataset into train set and test set.\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "#x = text\n",
        "#y = labels"
      ],
      "metadata": {
        "id": "t7uGbgg2P_Sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f3c488-a8c9-40ad-e394-42fc47e1818b",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(738,)\n",
            "(738,)\n",
            "(246,)\n",
            "(246,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df['Text']\n",
        "y = df['Sentiment']"
      ],
      "metadata": {
        "id": "1Lt5YDqcSMh2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-gram\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,3)) # Initialize vectorizer\n",
        "x_train_features = vectorizer.fit_transform(x)\n",
        "# x_test_features = vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "wVysZhaQSQoi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "x_oversampled, y_oversampled = oversample.fit_resample(x_train_features, y)"
      ],
      "metadata": {
        "id": "tL9EFzY1FxM5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Unvalidated data\n",
        "##### unigram\n",
        "\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.8484745413316842|0\\.9098677485838991|0\\.8404345867536123|0\\.9933508771929824|\n",
        "|1|0\\.1|0\\.9105029890744177|0\\.9409872306685416|0\\.9476166557687631|0\\.9360877192982455|\n",
        "|2|0\\.01|0\\.9033601319315606|0\\.9356093061872441|0\\.9474796070710656|0\\.9254035087719299|\n",
        "|3|0\\.001|0\\.8942279942279943|0\\.9300622773471605|0\\.9350463221643295|0\\.9267543859649121|\n",
        "|4|0\\.0001|0\\.8820140177283035|0\\.922532197804894|0\\.9224600519460265|0\\.9240877192982456|\n",
        "|5|1e-05|0\\.8810039167182024|0\\.922029353400361|0\\.9201066895112724|0\\.9254210526315789|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9501280353200883|0\\.9480888476435121|0\\.9719458269355498|0\\.9267368421052632|\n",
        "|1|0\\.1|0\\.9574525386313466|0\\.9558748625063916|0\\.9831689733542959|0\\.9307368421052631|\n",
        "|2|0\\.01|0\\.955448123620309|0\\.9535202234588036|0\\.9830977540315505|0\\.9267543859649121|\n",
        "|3|0\\.001|0\\.9541147902869757|0\\.9521389375216422|0\\.9831276444355851|0\\.9240877192982454|\n",
        "|4|0\\.0001|0\\.9534481236203091|0\\.9514400278207755|0\\.9831276444355851|0\\.9227543859649122|\n",
        "|5|1e-05|0\\.9534481236203091|0\\.9514400278207755|0\\.9831276444355851|0\\.9227543859649122|\n",
        "\n",
        "##### bigram\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.8048134405277263|0\\.8864505489024577|0\\.7998187019212091|0\\.9946666666666667|\n",
        "|1|0\\.1|0\\.890146361574933|0\\.9249702511083866|0\\.9573592422296999|0\\.8974561403508773|\n",
        "|2|0\\.01|0\\.8687693259121831|0\\.9075150445523669|0\\.9689925240289956|0\\.8561929824561403|\n",
        "|3|0\\.001|0\\.8657596371882086|0\\.9068860637028735|0\\.9569575001446088|0\\.8642105263157894|\n",
        "|4|0\\.0001|0\\.8688414759843331|0\\.9103896152391343|0\\.9440900716608864|0\\.881561403508772|\n",
        "|5|1e-05|0\\.8617295403009688|0\\.9062835908878745|0\\.9322961248763425|0\\.8842105263157893|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9567682119205297|0\\.9547695098550623|0\\.9797326308838352|0\\.9333859649122808|\n",
        "|1|0\\.1|0\\.9680750551876379|0\\.9664373933753148|0\\.9908051948051948|0\\.9453859649122809|\n",
        "|2|0\\.01|0\\.9641015452538632|0\\.9628292802819237|0\\.9857874952417205|0\\.942719298245614|\n",
        "|3|0\\.001|0\\.9627726269315673|0\\.9614191092941254|0\\.9857874952417205|0\\.9400701754385963|\n",
        "|4|0\\.0001|0\\.9621103752759381|0\\.9607005124547232|0\\.9855659082701337|0\\.9387543859649122|\n",
        "|5|1e-05|0\\.9621103752759381|0\\.9607005124547232|0\\.9855659082701337|0\\.9387543859649122|\n",
        "\n",
        "##### trigram\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.7926406926406926|0\\.8802654167233952|0\\.7888546642061078|0\\.9960000000000001|\n",
        "|1|0\\.1|0\\.8759224902082045|0\\.9133151627301638|0\\.964941665028552|0\\.8694561403508774|\n",
        "|2|0\\.01|0\\.8169758812615955|0\\.862384101426233|0\\.985647318013047|0\\.7722807017543859|\n",
        "|3|0\\.001|0\\.8362605648319933|0\\.878572032662607|0\\.9830594241723378|0\\.8002456140350876|\n",
        "|4|0\\.0001|0\\.8433415790558646|0\\.8871891637340376|0\\.9590258014133253|0\\.8308947368421051|\n",
        "|5|1e-05|0\\.8444238301381158|0\\.8910702790846907|0\\.9461668498406144|0\\.8455789473684211|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9647373068432671|0\\.9625604680861816|0\\.9946472446472446|0\\.934719298245614|\n",
        "|1|0\\.1|0\\.9760706401766006|0\\.9753499373431864|0\\.992030492030492|0\\.9600526315789473|\n",
        "|2|0\\.01|0\\.9760706401766004|0\\.975384633019776|0\\.9907692307692308|0\\.9613859649122807|\n",
        "|3|0\\.001|0\\.9687637969094922|0\\.9676604017054176|0\\.988102564102564|0\\.9493859649122808|\n",
        "|4|0\\.0001|0\\.9687637969094922|0\\.9676604017054176|0\\.988102564102564|0\\.9493859649122808|\n",
        "|5|1e-05|0\\.9680971302428256|0\\.9669892607658204|0\\.988102564102564|0\\.9480526315789474|\n",
        "\n",
        "##### 4-gram\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.787569573283859|0\\.877818938030709|0\\.7840721161703361|0\\.9973333333333333|\n",
        "|1|0\\.1|0\\.8504844361987219|0\\.8926213241418338|0\\.9684824915857642|0\\.8321403508771932|\n",
        "|2|0\\.01|0\\.7793135435992579|0\\.8284741097744662|0\\.9895755964686099|0\\.7190350877192984|\n",
        "|3|0\\.001|0\\.802721088435374|0\\.8490527922660849|0\\.9868668501939302|0\\.7522982456140351|\n",
        "|4|0\\.0001|0\\.8250773036487322|0\\.868941289840188|0\\.981353083069501|0\\.7869122807017543|\n",
        "|5|1e-05|0\\.8321789321789321|0\\.8770300477579459|0\\.9663329308718183|0\\.809578947368421|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9627682119205299|0\\.961243299781015|0\\.9855685722727976|0\\.9400526315789474|\n",
        "|1|0\\.1|0\\.9787373068432672|0\\.9785436644156977|0\\.9860543184885291|0\\.9720350877192981|\n",
        "|2|0\\.01|0\\.9774128035320089|0\\.9772829293587983|0\\.9859435589948582|0\\.9693684210526318|\n",
        "|3|0\\.001|0\\.9734172185430465|0\\.9729853069470735|0\\.9857962213225371|0\\.9613859649122809|\n",
        "|4|0\\.0001|0\\.9700927152317881|0\\.9693889682442052|0\\.9857962213225371|0\\.9547192982456141|\n",
        "|5|1e-05|0\\.9694304635761588|0\\.9687268852291812|0\\.9857629212892369|0\\.9534035087719298|\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F3MUeuKS4eSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_validate"
      ],
      "metadata": {
        "id": "8SIAP8GYm_wu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
        "scorers = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score)\n",
        "}\n",
        "mnb = MultinomialNB()\n",
        "classifier = GridSearchCV(mnb, parameters, return_train_score=False, cv=10, scoring=scorers, refit='accuracy')\n",
        "classifier.fit(x_train_features, y)\n",
        "# classifier.fit(x_oversampled, y_oversampled)"
      ],
      "metadata": {
        "id": "ntsQZqyfnKKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a972a181-a1a4-4c24-8b32-3af6102fb76b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, estimator=MultinomialNB(),\n",
              "             param_grid={'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 1e-05]},\n",
              "             refit='accuracy',\n",
              "             scoring={'accuracy': make_scorer(accuracy_score),\n",
              "                      'f1': make_scorer(f1_score),\n",
              "                      'precision': make_scorer(precision_score),\n",
              "                      'recall': make_scorer(recall_score)})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(classifier.cv_results_)\n",
        "results[['param_alpha', 'mean_test_accuracy', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall']]"
      ],
      "metadata": {
        "id": "3kkXVJ0va9hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "UHvBOSVckSPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = classifier.predict(x_train_features)\n",
        "tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
        "print(f\"True Negative: {tn} False Positive: {fp} False Negative: {fn} True Postive: {tp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivDVtof5itTk",
        "outputId": "c3f96e9d-b5dd-4580-d7e3-c58b529e277e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Negative: 232 False Positive: 0 False Negative: 2 True Postive: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = pd.read_csv('/content/thesis/Dataset/kaggle_tweets.csv') #read csv\n",
        "\n",
        "df_shape = corpus.shape # (row, column)\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df_shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', corpus.at[i, 'text']).lower()\n",
        "  corpus.at[i, 'text'] = re.sub(r'\\n', '', string)\n",
        "\n",
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "stopwords_tl  = set(stopwordsiso.stopwords('tl'))\n",
        "filtered_sentence = []\n",
        "filtered_sentence2 = []\n",
        "\n",
        "for i in range(len(corpus['text'])):\n",
        "  document = corpus.loc[i, 'text']\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens] # stem each words\n",
        "  filtered_sentence = [token for token in stemmed_tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  filtered_sentence2 = [token for token in filtered_sentence if not token in stopwords_tl] #remove tagalog stopwords\n",
        "\n",
        "  document = \" \".join(filtered_sentence2)\n",
        "  corpus.loc[i, 'text'] = document\n",
        "\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,4)) # Initialize vectorizer\n",
        "# x_train_features = vectorizer.fit_transform(x)\n",
        "import pickle\n",
        "\n",
        "pickled_vectorizer = pickle.load(open('/content/thesis/vectorizer.pkl', 'rb'))\n",
        "pickled_model = pickle.load(open('/content/thesis/model.pkl', 'rb'))\n",
        "x_test_features = pickled_vectorizer.transform(corpus['text'])\n",
        "res = pickled_model.predict(x_test_features)"
      ],
      "metadata": {
        "id": "xeAqUvOjKKrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text, sentiment in zip(corpus['text'], res):\n",
        "  print(sentiment, text)\n"
      ],
      "metadata": {
        "id": "v4aPb925M4gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(classifier, open('model.pkl', 'wb'))\n",
        "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "uW-TlfGz1az1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}