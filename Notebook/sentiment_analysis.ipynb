{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eeyvee-0x4d/cs-thesis/blob/main/Notebook/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdQ3Ic7Yagi"
      },
      "source": [
        "<h1>Clone repository</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6V5eVRbMm5e"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/eeyvee-0x4d/cs-thesis thesis\n",
        "%cd thesis\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhuqMaQU6Lwb"
      },
      "outputs": [],
      "source": [
        "!pip install stopwordsiso\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "import stopwordsiso\n",
        "\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXR9EvKZB0Y"
      },
      "source": [
        "<h1>Text Preprocessing</h1>\n",
        "<ul>\n",
        "  <li>Import dataset</li>\n",
        "  <li>Remove urls</li>\n",
        "  <li>Remove special characters</li>\n",
        "  <li>Convert text data to lowercase</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X-VQ9aBDLDM3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/trainingdata.csv') #read csv\n",
        "\n",
        "df_shape = df.shape # (row, column)\n",
        "\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df_shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', df.at[i, 'Text']).lower()\n",
        "  string = re.sub(r'[^a-zA-Z0-9 ]', '', string)\n",
        "  df.at[i, 'Text'] = re.sub(r'\\n', ' ', string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDyDPZA-VV9"
      },
      "source": [
        "<h1>Natural Language Toolkit NLTK</h1>\n",
        "<p>\n",
        "Nltk will be used to preprocess to corpus.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1TpyQH-1JW"
      },
      "source": [
        "<h1>Stemming words using Porter Stemmer</h1>\n",
        "<p>\n",
        "Apply porter stemmer to each tokens first then rebuild the tokens into sentence.\n",
        "</p>\n",
        "<hr>\n",
        "<h1>Stop words removal</h1>\n",
        "<p>\n",
        "Remove stop words in english and tagalog.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WyqpJpwL6lYO"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "stopwords_tl  = set(stopwordsiso.stopwords('tl'))\n",
        "filtered_sentence = []\n",
        "filtered_sentence2 = []\n",
        "\n",
        "for i in range(len(df['Text'])):\n",
        "  document = df.loc[i, 'Text']\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  filtered_sentence = [token for token in tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  filtered_sentence2 = [token for token in filtered_sentence if not token in stopwords_tl] #remove tagalog stopwords\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in filtered_sentence2] # stem each words\n",
        "\n",
        "  document = \" \".join(stemmed_tokens)\n",
        "  df.loc[i, 'Text'] = document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FN438rY_aOV"
      },
      "source": [
        "# Create n-grams from 1-4\n",
        "\n",
        "Create n-grams from 1 to 4 for exprementational purposes. Use params `ngram_range=(1,1)`, `ngram_range=(1,2)`, `ngram_range=(1,3)`, `ngram_range=(1,4)` in `TfidfVectorizer(ngram_range=(1,1))`.\n",
        "Default is `ngram_range=(1,1)`\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Perform TF-IDF to the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df['Text']\n",
        "y = df['Sentiment']"
      ],
      "metadata": {
        "id": "1Lt5YDqcSMh2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-gram\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 4)) # Initialize vectorizer\n",
        "x_train_features = vectorizer.fit_transform(x)\n",
        "# x_test_features = vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "wVysZhaQSQoi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "x_oversampled, y_oversampled = oversample.fit_resample(x_train_features, y)"
      ],
      "metadata": {
        "id": "tL9EFzY1FxM5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Unvalidated data\n",
        "##### unigram\n",
        "\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.8484745413316842|0\\.9098677485838991|0\\.8404345867536123|0\\.9933508771929824|\n",
        "|1|0\\.1|0\\.9105029890744177|0\\.9409872306685416|0\\.9476166557687631|0\\.9360877192982455|\n",
        "|2|0\\.01|0\\.9033601319315606|0\\.9356093061872441|0\\.9474796070710656|0\\.9254035087719299|\n",
        "|3|0\\.001|0\\.8942279942279943|0\\.9300622773471605|0\\.9350463221643295|0\\.9267543859649121|\n",
        "|4|0\\.0001|0\\.8820140177283035|0\\.922532197804894|0\\.9224600519460265|0\\.9240877192982456|\n",
        "|5|1e-05|0\\.8810039167182024|0\\.922029353400361|0\\.9201066895112724|0\\.9254210526315789|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9501280353200883|0\\.9480888476435121|0\\.9719458269355498|0\\.9267368421052632|\n",
        "|1|0\\.1|0\\.9574525386313466|0\\.9558748625063916|0\\.9831689733542959|0\\.9307368421052631|\n",
        "|2|0\\.01|0\\.955448123620309|0\\.9535202234588036|0\\.9830977540315505|0\\.9267543859649121|\n",
        "|3|0\\.001|0\\.9541147902869757|0\\.9521389375216422|0\\.9831276444355851|0\\.9240877192982454|\n",
        "|4|0\\.0001|0\\.9534481236203091|0\\.9514400278207755|0\\.9831276444355851|0\\.9227543859649122|\n",
        "|5|1e-05|0\\.9534481236203091|0\\.9514400278207755|0\\.9831276444355851|0\\.9227543859649122|\n",
        "\n",
        "##### bigram\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.8048134405277263|0\\.8864505489024577|0\\.7998187019212091|0\\.9946666666666667|\n",
        "|1|0\\.1|0\\.890146361574933|0\\.9249702511083866|0\\.9573592422296999|0\\.8974561403508773|\n",
        "|2|0\\.01|0\\.8687693259121831|0\\.9075150445523669|0\\.9689925240289956|0\\.8561929824561403|\n",
        "|3|0\\.001|0\\.8657596371882086|0\\.9068860637028735|0\\.9569575001446088|0\\.8642105263157894|\n",
        "|4|0\\.0001|0\\.8688414759843331|0\\.9103896152391343|0\\.9440900716608864|0\\.881561403508772|\n",
        "|5|1e-05|0\\.8617295403009688|0\\.9062835908878745|0\\.9322961248763425|0\\.8842105263157893|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9567682119205297|0\\.9547695098550623|0\\.9797326308838352|0\\.9333859649122808|\n",
        "|1|0\\.1|0\\.9680750551876379|0\\.9664373933753148|0\\.9908051948051948|0\\.9453859649122809|\n",
        "|2|0\\.01|0\\.9641015452538632|0\\.9628292802819237|0\\.9857874952417205|0\\.942719298245614|\n",
        "|3|0\\.001|0\\.9627726269315673|0\\.9614191092941254|0\\.9857874952417205|0\\.9400701754385963|\n",
        "|4|0\\.0001|0\\.9621103752759381|0\\.9607005124547232|0\\.9855659082701337|0\\.9387543859649122|\n",
        "|5|1e-05|0\\.9621103752759381|0\\.9607005124547232|0\\.9855659082701337|0\\.9387543859649122|\n",
        "\n",
        "##### trigram\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.7926406926406926|0\\.8802654167233952|0\\.7888546642061078|0\\.9960000000000001|\n",
        "|1|0\\.1|0\\.8759224902082045|0\\.9133151627301638|0\\.964941665028552|0\\.8694561403508774|\n",
        "|2|0\\.01|0\\.8169758812615955|0\\.862384101426233|0\\.985647318013047|0\\.7722807017543859|\n",
        "|3|0\\.001|0\\.8362605648319933|0\\.878572032662607|0\\.9830594241723378|0\\.8002456140350876|\n",
        "|4|0\\.0001|0\\.8433415790558646|0\\.8871891637340376|0\\.9590258014133253|0\\.8308947368421051|\n",
        "|5|1e-05|0\\.8444238301381158|0\\.8910702790846907|0\\.9461668498406144|0\\.8455789473684211|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9647373068432671|0\\.9625604680861816|0\\.9946472446472446|0\\.934719298245614|\n",
        "|1|0\\.1|0\\.9760706401766006|0\\.9753499373431864|0\\.992030492030492|0\\.9600526315789473|\n",
        "|2|0\\.01|0\\.9760706401766004|0\\.975384633019776|0\\.9907692307692308|0\\.9613859649122807|\n",
        "|3|0\\.001|0\\.9687637969094922|0\\.9676604017054176|0\\.988102564102564|0\\.9493859649122808|\n",
        "|4|0\\.0001|0\\.9687637969094922|0\\.9676604017054176|0\\.988102564102564|0\\.9493859649122808|\n",
        "|5|1e-05|0\\.9680971302428256|0\\.9669892607658204|0\\.988102564102564|0\\.9480526315789474|\n",
        "\n",
        "##### 4-gram\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.787569573283859|0\\.877818938030709|0\\.7840721161703361|0\\.9973333333333333|\n",
        "|1|0\\.1|0\\.8504844361987219|0\\.8926213241418338|0\\.9684824915857642|0\\.8321403508771932|\n",
        "|2|0\\.01|0\\.7793135435992579|0\\.8284741097744662|0\\.9895755964686099|0\\.7190350877192984|\n",
        "|3|0\\.001|0\\.802721088435374|0\\.8490527922660849|0\\.9868668501939302|0\\.7522982456140351|\n",
        "|4|0\\.0001|0\\.8250773036487322|0\\.868941289840188|0\\.981353083069501|0\\.7869122807017543|\n",
        "|5|1e-05|0\\.8321789321789321|0\\.8770300477579459|0\\.9663329308718183|0\\.809578947368421|\n",
        "\n",
        "##### Oversampled\n",
        "|index|param\\_alpha|mean\\_test_accuracy|mean\\_test_f1|mean\\_test_precision|mean\\_test_recall|\n",
        "|---|---|---|---|---|---|\n",
        "|0|1|0\\.9627682119205299|0\\.961243299781015|0\\.9855685722727976|0\\.9400526315789474|\n",
        "|1|0\\.1|0\\.9787373068432672|0\\.9785436644156977|0\\.9860543184885291|0\\.9720350877192981|\n",
        "|2|0\\.01|0\\.9774128035320089|0\\.9772829293587983|0\\.9859435589948582|0\\.9693684210526318|\n",
        "|3|0\\.001|0\\.9734172185430465|0\\.9729853069470735|0\\.9857962213225371|0\\.9613859649122809|\n",
        "|4|0\\.0001|0\\.9700927152317881|0\\.9693889682442052|0\\.9857962213225371|0\\.9547192982456141|\n",
        "|5|1e-05|0\\.9694304635761588|0\\.9687268852291812|0\\.9857629212892369|0\\.9534035087719298|\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F3MUeuKS4eSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
        "scorers = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score)\n",
        "}\n",
        "mnb = MultinomialNB()\n",
        "classifier = GridSearchCV(mnb, parameters, return_train_score=False, cv=10, scoring=scorers, refit='accuracy')\n",
        "# classifier.fit(x_train_features, y)\n",
        "classifier.fit(x_oversampled, y_oversampled)"
      ],
      "metadata": {
        "id": "ntsQZqyfnKKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(classifier.cv_results_)\n",
        "results[['param_alpha', 'mean_test_accuracy', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall']]"
      ],
      "metadata": {
        "id": "3kkXVJ0va9hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "UHvBOSVckSPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = classifier.predict(x_train_features)\n",
        "tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
        "print(f\"True Negative: {tn} False Positive: {fp} False Negative: {fn} True Postive: {tp}\")"
      ],
      "metadata": {
        "id": "ivDVtof5itTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = pd.read_csv('/content/thesis/Dataset/kaggle_tweets.csv') #read csv\n",
        "\n",
        "df_shape = corpus.shape # (row, column)\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df_shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', corpus.at[i, 'text']).lower()\n",
        "  corpus.at[i, 'text'] = re.sub(r'\\n', '', string)\n",
        "\n",
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "stopwords_tl  = set(stopwordsiso.stopwords('tl'))\n",
        "filtered_sentence = []\n",
        "filtered_sentence2 = []\n",
        "\n",
        "for i in range(len(corpus['text'])):\n",
        "  document = corpus.loc[i, 'text']\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens] # stem each words\n",
        "  filtered_sentence = [token for token in stemmed_tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  filtered_sentence2 = [token for token in filtered_sentence if not token in stopwords_tl] #remove tagalog stopwords\n",
        "\n",
        "  document = \" \".join(filtered_sentence2)\n",
        "  corpus.loc[i, 'text'] = document\n",
        "\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,4)) # Initialize vectorizer\n",
        "# x_train_features = vectorizer.fit_transform(x)\n",
        "import pickle\n",
        "\n",
        "pickled_vectorizer = pickle.load(open('/content/thesis/vectorizer.pkl', 'rb'))\n",
        "pickled_model = pickle.load(open('/content/thesis/model.pkl', 'rb'))\n",
        "x_test_features = pickled_vectorizer.transform(corpus['text'])\n",
        "res = pickled_model.predict(x_test_features)"
      ],
      "metadata": {
        "id": "xeAqUvOjKKrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text, sentiment in zip(corpus['text'], res):\n",
        "  print(sentiment, text)\n"
      ],
      "metadata": {
        "id": "v4aPb925M4gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(classifier, open('model.pkl', 'wb'))\n",
        "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "uW-TlfGz1az1"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}