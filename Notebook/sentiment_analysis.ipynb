{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eeyvee-0x4d/cs-thesis/blob/main/Notebook/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWdQ3Ic7Yagi"
      },
      "source": [
        "<h1>Clone repository</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9WC5LBymdt",
        "outputId": "bf655210-9e0c-43c2-9982-cc0f06739174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaFgOkofymd4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "\n",
        "df = pd.read_csv('/gdrive/MyDrive/vaccination_all_tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6U4f5en07He"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['id', 'user_description', 'user_created', 'user_followers', 'user_friends', 'user_favourites', 'user_verified', 'hashtags', 'source', 'retweets', 'favorites', 'is_retweet'])\n",
        "df.to_csv(path_or_buf='/content/tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEUhwLHG8JpF"
      },
      "outputs": [],
      "source": [
        "txtfile = open('/content/locations.txt', 'r')\n",
        "locations = txtfile.read().splitlines()\n",
        "\n",
        "df = pd.read_csv('/content/tweets.csv')\n",
        "tweets = df[df['user_location'].isin(locations)]\n",
        "tweets.to_csv(path_or_buf='/content/tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6V5eVRbMm5e",
        "outputId": "e083f074-832b-4458-c659-dc0d1e7aa735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'thesis'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (313/313), done.\u001b[K\n",
            "remote: Compressing objects: 100% (259/259), done.\u001b[K\n",
            "remote: Total 313 (delta 77), reused 219 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (313/313), 42.36 MiB | 14.26 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "/content/thesis\n",
            "Dataset  Notebook  README.md  StreamingAPI\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/eeyvee-0x4d/cs-thesis thesis\n",
        "%cd thesis\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RImCpBW1LApR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7jw9rEvakhs",
        "outputId": "8d444ad2-6c18-427d-846a-2141dc6ac19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sentiment                                               Text\n",
            "0   Negative    oh my god sakita sa akong bukton uy astrazeneca\n",
            "1   Negative  nakakapanghina ng katawan ang vaccine nagkaron...\n",
            "2   Positive  second dose done heart astrazeneca dbcvaccinat...\n",
            "3   Negative  same lang din side effect ng first at second d...\n",
            "4   Positive  booster shot done resbakuna aztrazeneca-pfizer...\n",
            "5   Positive  got boosted today  astrazeneca covid booster b...\n",
            "6   Positive  just got boosted let us fight covid variants t...\n",
            "7   Positive  booster done astrazeneca resbakuna booster htt...\n",
            "8   Positive  yay mama and papa got their first shot today r...\n",
            "9   Positive  doing my civic and moral duty to protect mysel...\n",
            "10  Positive  just got boosted let us fight covid variants  ...\n",
            "11  Positive  booster done astrazeneca resbakuna booster htt...\n",
            "12  Negative  ang sakit ng buong katawan ko muscles and butu...\n",
            "13  Negative  got my first dose yan tapang ka kanina lagnat ...\n",
            "14  Positive  done second dose salamat sa dios astrazeneca m...\n",
            "15  Negative  antukin ako pero yung antok ko ngayon kakaiba ...\n",
            "16  Negative            lalagnatin ako shet astrazeneca booster\n",
            "17  Positive                    got my booster shot astrazeneca\n",
            "18  Positive  done with my booster shot astrazeneca https://...\n",
            "19  Negative  eleven eleven mawala na ang side effect ng ast...\n",
            "20  Positive  I am done with my booster shot pa-booster na d...\n",
            "21  Positive  finally got my astrazeneca booster jab today h...\n",
            "22  Negative  ready na tayo lagnatin mamaya booster shot ast...\n",
            "23  Negative  para akong binugbog ni astrazeneca https://t.c...\n",
            "24  Negative  napaka traydor mo naman astrazeneca booster ht...\n",
            "25  Negative  that booster jab yesterday makes me sick at na...\n",
            "26  Negative  putang inang booster sama ng pakiramdam ko hig...\n",
            "27  Negative  para akong nakipag boxing ng three rounds saki...\n",
            "28  Negative  shems ramdam ko na ang sumpa ni zenki sana buk...\n",
            "29  Positive  na-booster na astrazeneca gid hahaha https://t...\n",
            "30  Negative  wala ka pang ten minutes ambigat mo na  ? Boos...\n",
            "31  Negative                    fever astrazeneca booster shots\n",
            "32  Positive  went to nbs after being vaccinated went out em...\n",
            "33  Positive  let us keep that protection longer get boosted...\n",
            "34  Negative  yung ang sama ng pakiramdam ko dahil  sa vacci...\n",
            "35  Negative  gagi from second dose to booster ng astrazenec...\n",
            "36  Negative  ah yawa sakit ng buong katawan ko astrazeneca ...\n",
            "37  Negative  ang sakit ng bewang ko sexbomb sexbomb sexbomb...\n",
            "38  Negative                   tamang seizures lang astrazeneca\n",
            "39  Negative  grabe naman adverse effects ng booster shot as...\n",
            "40  Positive  just got boosted let us fight covid variants  ...\n",
            "41  Negative                  kangalay booster haha astrazeneca\n",
            "42  Negative  hindi ako nilagnat pero sinipon naman ako boos...\n",
            "43  Negative  ang sakit ng braso ko dahil sa booster astraze...\n",
            "44  Negative                  saket ng astrazeneca booster dose\n",
            "45  Negative  normal ba ito naninilaw yung tinurukan ng baku...\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/Astra-December.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXR9EvKZB0Y"
      },
      "source": [
        "<h1>Text Preprocessing</h1>\n",
        "<ul>\n",
        "  <li>Import dataset</li>\n",
        "  <li>Remove urls</li>\n",
        "  <li>Remove special characters</li>\n",
        "  <li>Convert text data to lowercase</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X-VQ9aBDLDM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27acee9-aa5f-477d-dce8-de96dd35c87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sentiment                                               Text\n",
            "0   Negative    oh my god sakita sa akong bukton uy astrazeneca\n",
            "1   Negative  nakakapanghina ng katawan ang vaccine nagkaron...\n",
            "2   Positive  second dose done heart astrazeneca dbcvaccinat...\n",
            "3   Negative  same lang din side effect ng first at second d...\n",
            "4   Positive    booster shot done resbakuna aztrazeneca-pfizer \n",
            "5   Positive  got boosted today  astrazeneca covid booster b...\n",
            "6   Positive  just got boosted let us fight covid variants t...\n",
            "7   Positive        booster done astrazeneca resbakuna booster \n",
            "8   Positive  yay mama and papa got their first shot today r...\n",
            "9   Positive  doing my civic and moral duty to protect mysel...\n",
            "10  Positive  just got boosted let us fight covid variants  ...\n",
            "11  Positive        booster done astrazeneca resbakuna booster \n",
            "12  Negative  ang sakit ng buong katawan ko muscles and butu...\n",
            "13  Negative  got my first dose yan tapang ka kanina lagnat ...\n",
            "14  Positive  done second dose salamat sa dios astrazeneca m...\n",
            "15  Negative  antukin ako pero yung antok ko ngayon kakaiba ...\n",
            "16  Negative            lalagnatin ako shet astrazeneca booster\n",
            "17  Positive                    got my booster shot astrazeneca\n",
            "18  Positive             done with my booster shot astrazeneca \n",
            "19  Negative  eleven eleven mawala na ang side effect ng ast...\n",
            "20  Positive  i am done with my booster shot pa-booster na d...\n",
            "21  Positive      finally got my astrazeneca booster jab today \n",
            "22  Negative  ready na tayo lagnatin mamaya booster shot ast...\n",
            "23  Negative                para akong binugbog ni astrazeneca \n",
            "24  Negative       napaka traydor mo naman astrazeneca booster \n",
            "25  Negative  that booster jab yesterday makes me sick at na...\n",
            "26  Negative  putang inang booster sama ng pakiramdam ko hig...\n",
            "27  Negative  para akong nakipag boxing ng three rounds saki...\n",
            "28  Negative  shems ramdam ko na ang sumpa ni zenki sana buk...\n",
            "29  Positive              na-booster na astrazeneca gid hahaha \n",
            "30  Negative  wala ka pang ten minutes ambigat mo na  ? boos...\n",
            "31  Negative                    fever astrazeneca booster shots\n",
            "32  Positive  went to nbs after being vaccinated went out em...\n",
            "33  Positive  let us keep that protection longer get boosted...\n",
            "34  Negative  yung ang sama ng pakiramdam ko dahil  sa vacci...\n",
            "35  Negative  gagi from second dose to booster ng astrazenec...\n",
            "36  Negative  ah yawa sakit ng buong katawan ko astrazeneca ...\n",
            "37  Negative  ang sakit ng bewang ko sexbomb sexbomb sexbomb...\n",
            "38  Negative                   tamang seizures lang astrazeneca\n",
            "39  Negative  grabe naman adverse effects ng booster shot as...\n",
            "40  Positive  just got boosted let us fight covid variants  ...\n",
            "41  Negative                  kangalay booster haha astrazeneca\n",
            "42  Negative  hindi ako nilagnat pero sinipon naman ako boos...\n",
            "43  Negative  ang sakit ng braso ko dahil sa booster astraze...\n",
            "44  Negative                  saket ng astrazeneca booster dose\n",
            "45  Negative  normal ba ito naninilaw yung tinurukan ng baku...\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/thesis/Dataset/Astra-December.csv') #read csv\n",
        "\n",
        "df_shape = df.shape # (row, column)\n",
        "\n",
        "# remove urls, remove special chars, conver to lowercase\n",
        "for i in range(df_shape[0]):\n",
        "  string = re.sub(r'http\\S+', '', df.at[i, 'Text']).lower()\n",
        "  df.at[i, 'Text'] = re.sub(r'\\n', '', string)\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDyDPZA-VV9"
      },
      "source": [
        "<h1>Natural Language Toolkit NLTK</h1>\n",
        "<p>\n",
        "Nltk will be used to preprocess to corpus.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhuqMaQU6Lwb",
        "outputId": "3c3ef672-9987-485f-b12c-3b1110b8989b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stopwordsiso\n",
            "  Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▌                           | 10 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 40 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "!pip install stopwordsiso\n",
        "\n",
        "import nltk\n",
        "import stopwordsiso\n",
        "\n",
        "from nltk.stem import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1TpyQH-1JW"
      },
      "source": [
        "<h1>Stemming words using Porter Stemmer</h1>\n",
        "<p>\n",
        "Apply porter stemmer to each tokens first then rebuild the tokens into sentence.\n",
        "</p>\n",
        "<hr>\n",
        "<h1>Stop words removal</h1>\n",
        "<p>\n",
        "Remove stop words in english and tagalog.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WyqpJpwL6lYO"
      },
      "outputs": [],
      "source": [
        "corpus = df[\"Text\"]\n",
        "\n",
        "stemmer = PorterStemmer() # Porter Stemmer\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english')) # English stopwords\n",
        "stopwords_tl  = set(stopwordsiso.stopwords('tl'))\n",
        "filtered_sentence = []\n",
        "filtered_sentence2 = []\n",
        "\n",
        "for i in range(len(df['Text'])):\n",
        "  document = df.loc[i, 'Text']\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens] # stem each words\n",
        "  filtered_sentence = [token for token in stemmed_tokens if not token in stopwords_eng] # remove english stopwords\n",
        "  filtered_sentence2 = [token for token in filtered_sentence if not token in stopwords_tl] #remove tagalog stopwords\n",
        "\n",
        "  document = \" \".join(filtered_sentence2)\n",
        "  df.loc[i, 'Text'] = document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FN438rY_aOV"
      },
      "source": [
        "# Create train data and test data\n",
        "\n",
        "Create train data and test data using 2:1 ration. Use `train_test_split` for this.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Create n-grams from 1-4 ?\n",
        "\n",
        "Create n-grams from 1 to 4 for exprementational purposes. Use params `ngram_range=(1,1)`, `ngram_range=(1,2)`, `ngram_range=(1,3)`, `ngram_range=(1,4)` in `TfidfVectorizer(ngram_range=(1,1))`.\n",
        "Default is `ngram_range=(1,1)`\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Perform TF-IDF to the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "wKLdAMszBeEB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment']) # split dataset into train set and test set.\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "#x = text\n",
        "#y = labels"
      ],
      "metadata": {
        "id": "t7uGbgg2P_Sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39038935-c954-4486-bc38-d0821fd3e790"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34,)\n",
            "(34,)\n",
            "(12,)\n",
            "(12,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer() # Initialize vectorizer\n",
        "x_train_features = vectorizer.fit_transform(x_train)\n",
        "x_test_features = vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "wVysZhaQSQoi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "8SIAP8GYm_wu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MultinomialNB()\n",
        "classifier.fit(x_train_features, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntsQZqyfnKKr",
        "outputId": "8f44d842-6052-4bfd-b537-aafd5f4ee4ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = classifier.predict(x_test_features)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(pred, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kkXVJ0va9hw",
        "outputId": "5914ae55-b228-4602-c99a-427d3f6f9464"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}